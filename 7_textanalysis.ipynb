{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9llKiqGJpIwX",
        "outputId": "47fcadf0-ca5a-40ca-dc87-c50d83558eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
      ],
      "metadata": {
        "id": "GJ74N7m3pehI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized_text= sent_tokenize(text)\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1gcKL8npz4M",
        "outputId": "e4b81189-c6fb-459d-ca99-74ae698ce8b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJA-WD7Rp3qL",
        "outputId": "a58b2393-faa8-4199-a739-53e7d3643e31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(tokenized_word)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnlZZzzJp7FA",
        "outputId": "db9f0a4a-6f96-4032-8ff9-d30709061483"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('The', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('a', 'DT'), ('text', 'NN'), ('paragraph', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('Tokenization', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "\n",
        "print(\"*******\")\n",
        "\n",
        "filtered_tokens = [token for token in tokenized_word if token.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-km_9hRrqCO4",
        "outputId": "159cf00f-deb2-412d-c24c-4e87a4e0907c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'theirs', 'be', 'whom', 'through', 'with', 'are', 'been', \"it's\", 'off', 'nor', \"weren't\", 'and', 'above', 'they', 'my', 'if', 'out', 'there', 'our', 'of', 'needn', 'shouldn', 'he', 'again', 'not', 'same', 't', \"mightn't\", 'couldn', 'such', \"doesn't\", 'in', 'so', 'ma', 'ourselves', 'as', 'who', 'your', 'all', 'didn', 'those', 'or', 'this', \"won't\", 'too', 'now', 'over', 'because', 'yourself', 'her', 'it', 'both', 'most', 'from', 'during', 'only', \"she's\", 'some', \"wasn't\", 'was', 'that', 'mustn', 'shan', 'further', 'why', 'itself', 'o', 'at', 'doing', 'has', 'his', 'under', 'how', 'each', \"you've\", \"don't\", 're', 'herself', 'but', 'its', 'ain', 'down', 'can', 'their', \"you'll\", 'very', 'did', 'just', \"didn't\", \"hadn't\", 'into', 'mightn', 'for', 'we', 'a', 'after', 'about', 'against', 'weren', 'she', 'than', 'have', 'while', 'no', 's', \"you'd\", 'me', 'by', \"needn't\", 'myself', 'once', \"isn't\", 'before', 'other', 'haven', 'had', 'these', \"that'll\", 'an', 'below', 'wasn', 'which', 'ours', 've', 'on', 'here', 'y', 'won', \"you're\", \"aren't\", \"couldn't\", 'where', 'being', \"wouldn't\", 'you', 'what', 'them', \"mustn't\", 'were', 'm', 'hers', 'yourselves', 'when', 'aren', 'up', 'wouldn', 'do', 'him', 'don', \"hasn't\", \"haven't\", 'll', 'does', 'between', 'hasn', \"shan't\", 'having', \"shouldn't\", 'until', 'few', 'i', 'own', \"should've\", 'is', 'himself', 'any', 'yours', 'then', 'd', 'hadn', 'isn', 'themselves', 'the', 'will', 'am', 'should', 'more', 'doesn', 'to'}\n",
            "*******\n",
            "['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4epBSGCqW8z",
        "outputId": "ad9cab36-76f7-4be0-d744-e4b9b70c4399"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['token', 'first', 'step', 'text', 'analyt', '.', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentenc', 'call', 'token', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx8_O3DNq2z9",
        "outputId": "a884ee4e-7f75-46a4-acd4-369d60cb516d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentence', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Tokens:\", tokenized_word)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "print(\"Filtered Tokens (Stop Words Removal):\", filtered_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xJoJP4BrGOm",
        "outputId": "368e54bd-bcce-43d8-f507-5ad2a1495a12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n",
            "POS Tags: [('Tokenization', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('The', 'DT'), ('process', 'NN'), ('of', 'IN'), ('breaking', 'VBG'), ('down', 'RP'), ('a', 'DT'), ('text', 'NN'), ('paragraph', 'NN'), ('into', 'IN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), ('is', 'VBZ'), ('called', 'VBN'), ('Tokenization', 'NN'), ('.', '.')]\n",
            "Filtered Tokens (Stop Words Removal): ['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunks', 'words', 'sentences', 'called', 'Tokenization', '.']\n",
            "Stemmed Tokens: ['token', 'first', 'step', 'text', 'analyt', '.', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentenc', 'call', 'token', '.']\n",
            "Lemmatized Tokens: ['Tokenization', 'first', 'step', 'text', 'analytics', '.', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentence', 'called', 'Tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "HTUme8NsxmmU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = 'Jupiter is the largest Planet'\n",
        "documentB = 'Mars is the fourth planet from the Sun'"
      ],
      "metadata": {
        "id": "ENLVam_5ysgA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "metadata": {
        "id": "pO6Yu-Dty2oq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n"
      ],
      "metadata": {
        "id": "2ApyDCuby6Hn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "  numOfWordsA[word] += 1\n",
        "  numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "  numOfWordsB[word] += 1\n"
      ],
      "metadata": {
        "id": "VW6dZ-vMy8Jp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "  tfDict = {}\n",
        "  bagOfWordsCount = len(bagOfWords)\n",
        "  for word, count in wordDict.items():\n",
        "   tfDict[word] = count / float(bagOfWordsCount)\n",
        "  return tfDict \n",
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)\n"
      ],
      "metadata": {
        "id": "3tnfUo4Zy_9S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwGhTdCbzH4S",
        "outputId": "34ba0e64-c330-4502-bc46-4258b232202e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'from': 0.0,\n",
              " 'largest': 0.2,\n",
              " 'Planet': 0.2,\n",
              " 'fourth': 0.0,\n",
              " 'Mars': 0.0,\n",
              " 'the': 0.2,\n",
              " 'planet': 0.0,\n",
              " 'is': 0.2,\n",
              " 'Sun': 0.0,\n",
              " 'Jupiter': 0.2}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_wIdKNbzRf0",
        "outputId": "585f90a7-5ae4-43c1-c73a-b72313231da9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'from': 0.125,\n",
              " 'largest': 0.0,\n",
              " 'Planet': 0.0,\n",
              " 'fourth': 0.125,\n",
              " 'Mars': 0.125,\n",
              " 'the': 0.25,\n",
              " 'planet': 0.125,\n",
              " 'is': 0.125,\n",
              " 'Sun': 0.125,\n",
              " 'Jupiter': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def computeIDF(documents):\n",
        "    N = len(documents)\n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "\n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])"
      ],
      "metadata": {
        "id": "l8Krc7wyzTyb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGxQw9LPzsfd",
        "outputId": "993511b0-9c8f-4547-91d9-4501f8cf3165"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'from': 0.6931471805599453,\n",
              " 'largest': 0.6931471805599453,\n",
              " 'Planet': 0.6931471805599453,\n",
              " 'fourth': 0.6931471805599453,\n",
              " 'Mars': 0.6931471805599453,\n",
              " 'the': 0.0,\n",
              " 'planet': 0.6931471805599453,\n",
              " 'is': 0.0,\n",
              " 'Sun': 0.6931471805599453,\n",
              " 'Jupiter': 0.6931471805599453}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf\n",
        "\n",
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FKQ3i_Jz1mQ",
        "outputId": "a343d391-12a1-46c2-bc34-906c89077f4a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       from   largest    Planet    fourth      Mars  the    planet   is  \\\n",
            "0  0.000000  0.138629  0.138629  0.000000  0.000000  0.0  0.000000  0.0   \n",
            "1  0.086643  0.000000  0.000000  0.086643  0.086643  0.0  0.086643  0.0   \n",
            "\n",
            "        Sun   Jupiter  \n",
            "0  0.000000  0.138629  \n",
            "1  0.086643  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m37J0Kc90OpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}